import requests
import json
from datetime import datetime
import pandas as pd
from pyspark.sql.functions import to_date, max as spark_max, lit
from pyspark.sql import Row

# Load table with register URLs
df = spark.read.table("Clue_Data_Lakehouse.API_Register_Names")

# Get max timestamp
max_date = df.select(spark_max("timestamp")).collect()[0][0]

# Filter records with max timestamp
filtered_df = df.filter(to_date(df.timestamp) == to_date(lit(max_date))).select("Register_Records_URL")
url_list = filtered_df.toPandas()

# API setup
base_url = "https://nhs.clue.co.uk/clue/API/v2"
headers = {
    "Api_key": "xxxxxxxxxxxxxxxxxxxхххххххх",
    "Accept": "application/json",
    "Api_user": "аріххххххххх"
}

# Output collectors
records = []
errors = []

# Process URLs
for i, row in url_list.iterrows():
    partial_url = row["Register_Records_URL"]
    full_url = f"{base_url}{partial_url}"

    try:
        response = requests.get(full_url, headers=headers)
        response.raise_for_status()

        json_data = response.json()

        # Skip empty or bad responses
        if not json_data or (isinstance(json_data, dict) and not json_data.keys()):
            print(f"Empty response at: {partial_url}")
            errors.append(Row(
                url=partial_url,
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                error="Empty or invalid response"
            ))
            continue

        # Add metadata
        json_data["source_url"] = partial_url
        json_data["fetched_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        records.append(Row(**json_data))

    except Exception as e:
        errors.append(Row(
            url=partial_url,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            error=str(e)
        ))

# Convert to Spark DataFrames and save
if records:
    records_df = spark.createDataFrame(records)
    records_df.write.mode("overwrite").saveAsTable("Clue_Data_Lakehouse.API_Records_Full")
    print(f"Saved {records_df.count()} records to 'API_Records_Full'")
else:
    print("No records fetched successfully.")

if errors:
    errors_df = spark.createDataFrame(errors)
    errors_df.write.mode("overwrite").saveAsTable("Clue_Data_Lakehouse.API_Fetch_Errors")
    print(f"Logged {errors_df.count()} errors to 'API_Fetch_Errors'")
else:
    print("No errors encountered.")