
import requests
import json
from datetime import datetime
import pandas as pd
from pyspark.sql.functions import to_date, max as spark_max

# Load table with register URLs
df = spark.read.table("Clue_Data_Lakehouse.API_Register_Names")

# Find the maximum timestamp
max_date = df.select(spark_max("timestamp")).collect()[0][0]

# Display (optional)
print("Max timestamp found:", max_date)

# Filter to rows with the max timestamp
filtered_df = df.filter(to_date(df.timestamp) == to_date(lit(max_date))).select("Register_Records_URL")

# Convert to pandas for iteration
url_list = filtered_df.toPandas()

# API setup
base_url = "https://nhs.clue.co.uk/clue/API/v2"
headers = {
    "Api_key": "xxxxxxxxxxxxxxxxxxxхххххххх",
    "Accept": "application/json",
    "Api_user": "аріххххххххх"
    # "x-clue-pagesize": "1000"  # Optional
}

# Output location
output_file = "/lakehouse/default/Files/raw/full_json_data/full_api_data_Reg_RecIdl_test.json"

# Exception list
exceptions = []

# Save data
with open(output_file, "w", encoding="utf-8") as f:
    for i, row in url_list.iterrows():
        partial_url = row["Register_Records_URL"]
        full_url = f"{base_url}{partial_url}"

        try:
            response = requests.get(full_url, headers=headers)
            response.raise_for_status()

            json_data = response.json()

            # Skip if response is empty or doesn't contain useful content
            if not json_data or (isinstance(json_data, dict) and not json_data.keys()):
                print(f"Empty response at: {partial_url}")
                continue

            # Add source metadata
            json_data["source_url"] = partial_url

            # Write to file
            f.write(json.dumps(json_data) + "\n")

        except Exception as e:
            error_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{error_time}] Error fetching {partial_url}: {e}")
            exceptions.append({
                "timestamp": error_time,
                "url": partial_url,
                "error": str(e)
            })

# Optionally: convert exceptions to DataFrame or log
if exceptions:
    error_df = pd.DataFrame(exceptions)
    error_df.to_csv("/lakehouse/default/Files/logs/api_fetch_errors.csv", index=False)
    print(f"{len(exceptions)} errors logged.")
else:
    print("All URLs fetched successfully.")



